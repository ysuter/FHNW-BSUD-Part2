{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysuter/FHNW-BSUD-Part2/blob/main/L6-InformationExtraction/L6-InformationExtraction/L06_Text_Preprocessing_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXNG6sFHDgqr"
      },
      "source": [
        "# L06 Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1cejhXMCohK"
      },
      "source": [
        "### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "d1533e5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3f5996-5965-4f88-86a4-a3768d87bf9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# --- Setup for Google Colab ---\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1f303d33"
      },
      "outputs": [],
      "source": [
        "# --- Tokenization Setup ---\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Tokenizes a string into words using NLTK's Punkt tokenizer.\"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "15f98525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5ab008-0281-40aa-c3ec-205cc6f4d35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1 tokens = ['Founder', 'of', 'the', 'new', 'tech', 'company', 'FUIT', 'announces', 'headquarter-to-be', ':', 'New', 'York']\n"
          ]
        }
      ],
      "source": [
        "# --- Example 1 ---\n",
        "D1 = \"Founder of the new tech company FUIT announces headquarter-to-be: New York\"\n",
        "tokens = tokenize(D1)\n",
        "print(\"D1 tokens =\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5b9ba29b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622fa4c8-af2e-4cfa-e221-6e9d319ef331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D2 tokens = ['FUIT', 'have', 'changed', 'their', 'headquarter', 'from', 'New', 'York', 'to', 'Zurich']\n"
          ]
        }
      ],
      "source": [
        "# --- Example 2 ---\n",
        "D2 = \"FUIT have changed their headquarter from New York to Zurich\"\n",
        "tokens = tokenize(D2)\n",
        "print(\"D2 tokens =\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAMbCmz7Cw-z"
      },
      "source": [
        "## Text Preprocessing - Stop Word Removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6N7rZw2xC4fv"
      },
      "outputs": [],
      "source": [
        "# Define the documents\n",
        "D1 = \"FUIT offers very special IT services to their clients.\"\n",
        "D2 = \"Yesterday, all train services between London center and Heathrow airport had to be canceled.\"\n",
        "\n",
        "# Define the query\n",
        "query = \"IT services\"\n",
        "\n",
        "# Define the list of stop words\n",
        "stop_words = {\"a\", \"the\", \"of\", \"to\", \"and\", \"in\", \"for\", \"is\", \"that\", \"on\", \"it\", \"with\", \"by\"}\n",
        "\n",
        "# Function to preprocess text (convert to lower case and remove stop words)\n",
        "def preprocess(text):\n",
        "    words = text.lower().split()\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "# Preprocess the documents and the query\n",
        "processed_D1 = preprocess(D1)\n",
        "processed_D2 = preprocess(D2)\n",
        "processed_query = preprocess(query)\n",
        "\n",
        "# Function to check if all query words are in the document\n",
        "def contains_all_words(document, query):\n",
        "    return all(word in document for word in query)\n",
        "\n",
        "# Check which documents contain all query words\n",
        "retrieved_documents = []\n",
        "if contains_all_words(processed_D1, processed_query):\n",
        "    retrieved_documents.append(\"D1\")\n",
        "if contains_all_words(processed_D2, processed_query):\n",
        "    retrieved_documents.append(\"D2\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dI0HeunGIT2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96464098-56ed-40f1-aaec-b437c9849ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Documents: ['D1', 'D2']\n",
            "processed D1 ['fuit', 'offers', 'very', 'special', 'services', 'their', 'clients.']\n",
            "processed D2 ['yesterday,', 'all', 'train', 'services', 'between', 'london', 'center', 'heathrow', 'airport', 'had', 'be', 'canceled.']\n",
            "processed query ['services']\n"
          ]
        }
      ],
      "source": [
        "# Output the result\n",
        "print(\"Retrieved Documents:\", retrieved_documents)\n",
        "print('processed D1', processed_D1)\n",
        "print('processed D2', processed_D2)\n",
        "print('processed query', processed_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll3TvzHvJnmJ"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BcPAH3hzJd2V"
      },
      "outputs": [],
      "source": [
        "# Define the original and stemmed documents\n",
        "D1 = \"Wandern im Schweizer Jura. Die schönsten Routen und besten Einkehrmöglichkeiten\"\n",
        "D1_stemmed = \"wand im schweiz Jura. Die schon rout und best einkehrmog\"\n",
        "D2 = \"So verschönern Sie Ihre Wände: Wandschmuck und die neusten Tapetenmuster\"\n",
        "D2_stemmed = \"so verschon sie ihr wand: wandschmuck und die neust tapetenmust\"\n",
        "\n",
        "# Split the documents into words\n",
        "D1_words = D1.split()\n",
        "D1_stemmed_words = D1_stemmed.split()\n",
        "D2_words = D2.split()\n",
        "D2_stemmed_words = D2_stemmed.split()\n",
        "\n",
        "# Create a dictionary to map stemmed words to original words for D1 and D2\n",
        "stem_to_original_D1 = {stem: original for stem, original in zip(D1_stemmed_words, D1_words)}\n",
        "stem_to_original_D2 = {stem: original for stem, original in zip(D2_stemmed_words, D2_words)}\n",
        "\n",
        "# Find common stemmed words between D1 and D2\n",
        "common_stems = set(D1_stemmed_words) & set(D2_stemmed_words)\n",
        "\n",
        "# Identify overstemming examples\n",
        "overstemming_examples = []\n",
        "for stem in common_stems:\n",
        "    if stem_to_original_D1[stem] != stem_to_original_D2[stem]:\n",
        "        overstemming_examples.append((stem_to_original_D1[stem], stem_to_original_D2[stem]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tlKLs43ZJsWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1717f177-e645-46d7-c85a-bbed55bd2900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1 words: ['Wandern', 'im', 'Schweizer', 'Jura.', 'Die', 'schönsten', 'Routen', 'und', 'besten', 'Einkehrmöglichkeiten']\n",
            "D1 stemmed words: ['wand', 'im', 'schweiz', 'Jura.', 'Die', 'schon', 'rout', 'und', 'best', 'einkehrmog']\n",
            "D2 words: ['So', 'verschönern', 'Sie', 'Ihre', 'Wände:', 'Wandschmuck', 'und', 'die', 'neusten', 'Tapetenmuster']\n",
            "D2 stemmed words: ['so', 'verschon', 'sie', 'ihr', 'wand:', 'wandschmuck', 'und', 'die', 'neust', 'tapetenmust']\n"
          ]
        }
      ],
      "source": [
        "# Output the result\n",
        "print(\"D1 words:\", D1_words)\n",
        "print(\"D1 stemmed words:\", D1_stemmed_words)\n",
        "print(\"D2 words:\", D2_words)\n",
        "print(\"D2 stemmed words:\", D2_stemmed_words)\n",
        "for example in overstemming_examples:\n",
        "    print(f\"Overstemming example: {example[0]},{example[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkiQbHTLMZ6j"
      },
      "source": [
        "### Using the NLTK PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FfTju-m5PTdx"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def stemming(input_text):\n",
        "    \"\"\"\n",
        "    Applies stemming to the input text after removing stop words and punctuation.\n",
        "\n",
        "    Args:\n",
        "        input_text: The input text string.\n",
        "\n",
        "    Returns:\n",
        "        A list of stemmed words.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    words = word_tokenize(input_text)\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Perform stemming on each word\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return stemmed_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_QiYsS33Qr8u"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def stemming_German(input_text):\n",
        "  \"\"\"Applies stemming to the input text.\n",
        "\n",
        "  Args:\n",
        "    input_text: The input text string.\n",
        "\n",
        "  Returns:\n",
        "    A stemmed string.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the Snowball Stemmer for German\n",
        "  stemmer = SnowballStemmer(\"german\")\n",
        "\n",
        "  # Tokenize the input text\n",
        "  tokens = word_tokenize(input_text, language=\"german\")\n",
        "\n",
        "  # Remove punctuation and stop words\n",
        "  stop_words = set(stopwords.words('german'))\n",
        "  tokens = [token for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
        "\n",
        "  # Stem the tokens and join them back into a string\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "  stemmed_text = \" \".join(stemmed_tokens)\n",
        "\n",
        "  return stemmed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Mp7dxHCJLouZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d7ca5c-47c0-4b38-f1d2-f237eed73607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1 stemmed = ['wandern', 'im', 'schweizer', 'jura', 'die', 'schönsten', 'routen', 'und', 'besten', 'einkehrmöglichkeiten']\n",
            "D2 stemmed = ['verschönern', 'sie', 'ihr', 'wände', 'wandschmuck', 'und', 'die', 'neusten', 'tapetenmust']\n"
          ]
        }
      ],
      "source": [
        "D1_stemmed2 = stemming(D1)\n",
        "D2_stemmed2 = stemming(D2)\n",
        "print('D1 stemmed =', D1_stemmed2)\n",
        "print('D2 stemmed =', D2_stemmed2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "d435e5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860e2ce6-0121-45fb-bd54-333eef254175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1G    = This is an example sentence with stemming applied.\n",
            "stemmed ['exampl', 'sentenc', 'stem', 'appli']\n"
          ]
        }
      ],
      "source": [
        "# --- English stemming example ---\n",
        "D1G = \"This is an example sentence with stemming applied.\"\n",
        "stemmed_text = stemming(D1G)\n",
        "print('D1G    =', D1G)\n",
        "print('stemmed', stemmed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dgwsQmBmOQmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d767a8-f0c4-45f2-885e-d0c99fe3f90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D1    =  Wandern im Schweizer Jura. Die schönsten Routen und besten Einkehrmöglichkeiten.\n",
            "stemmed:  wand schweiz jura schon rout best einkehrmog\n"
          ]
        }
      ],
      "source": [
        "D1 = \"Wandern im Schweizer Jura. Die schönsten Routen und besten Einkehrmöglichkeiten.\"\n",
        "stemmed_text = stemming_German(D1)\n",
        "print('D1    = ',D1)\n",
        "print('stemmed: ',stemmed_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}